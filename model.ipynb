{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes, self.class_to_idx = self._find_classes(self.root_dir)\n",
    "        self.imgs = self._make_dataset()\n",
    "\n",
    "    def _find_classes(self, dir):\n",
    "        \"\"\"\n",
    "        Finds the class folders in a dataset.\n",
    "        \"\"\"\n",
    "        classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        classes.sort()\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def _make_dataset(self):\n",
    "        \"\"\"\n",
    "        Creates a list of samples with their class indices.\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        for target in sorted(self.class_to_idx.keys()):\n",
    "            d = os.path.join(self.root_dir, target)\n",
    "            if not os.path.isdir(d):\n",
    "                continue\n",
    "            for root, _, fnames in sorted(os.walk(d)):\n",
    "                for fname in sorted(fnames):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, self.class_to_idx[target])\n",
    "                    images.append(item)\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.imgs[idx]\n",
    "        # Using cv2 to read and process images\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example transform function that can be used with the CustomDataset\n",
    "def transform(image):\n",
    "    # Resize image to 224x224\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # Randomly flip image horizontally\n",
    "    if np.random.rand() > 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    # Convert image to PyTorch tensor and scale to [0,1] (it helps to normalize, Numerical Stability)\n",
    "    # Matching Activation Functions: Many neural network architectures use activation functions like sigmoid or tanh in their layers. \n",
    "    # These functions squeeze their input values into the range [0,1][0,1] \n",
    "    # Pre-trained Model Compatibility: Many pre-trained models (e.g., models available in torchvision or TensorFlow model libraries) \n",
    "    # expect input images to be scaled to [0,1][0,1]\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "train_dataset = CustomDataset(root_dir='data/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Now you can iterate over train_loader in your training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture explanation\n",
    "\n",
    "The architecture of a Convolutional Neural Network (CNN) typically involves a series of convolutional layers, pooling layers, and fully connected layers. The specific numbers of each type of layer and their configurations can be chosen based on empirical evidence, computational resources, and the complexity of the task at hand. Here's the rationale for the architecture I provided:\n",
    "\n",
    "    Convolutional Layers:\n",
    "        These layers are the core building blocks of a CNN. They apply a number of filters (also known as kernels) to the input to create feature maps. These feature maps capture spatial hierarchies and features (e.g., edges, textures) from the input images. Increasing the number of convolutional layers allows the network to learn more complex patterns. As we go deeper into the network, the filters can capture higher-level features (e.g., shapes, specific parts of an object).\n",
    "\n",
    "    Number of Filters:\n",
    "        The number of filters in each convolutional layer often increases with the depth of the network. This is because the complexity and the number of high-level features tend to increase as you move deeper, and more filters are needed to capture this complexity. Common practice is to double the number of filters after each pooling layer, which helps to balance the reduction in spatial dimensions.\n",
    "\n",
    "    Pooling Layers:\n",
    "        Pooling layers (in this case, max pooling) are used to reduce the spatial size of the representation, which decreases the number of parameters and computation in the network. This also helps to make the detection of features somewhat invariant to scale and orientation changes. Pooling helps to control overfitting by providing an abstracted form of the representation.\n",
    "\n",
    "    Fully Connected Layers:\n",
    "        After the convolutional and pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. The last fully connected layer (which is often called the \"output layer\") has as many neurons as there are classes in the dataset for a classification task. Each neuron in this layer will correspond to a class score.\n",
    "\n",
    "    Activation Functions:\n",
    "        The ReLU activation function is used after each convolution operation to introduce non-linear properties to the system, allowing the network to learn more complex functions.\n",
    "\n",
    "The architecture I described is a simple and somewhat generic model for illustrative purposes. In practice, the architecture might need adjustments. For instance, if the images are larger, you may have additional layers to further reduce the spatial dimensions before flattening for the fully connected layers. If there are more or fewer classes, or if the images are more or less complex, you would adjust the depth of the network and the number of neurons in the fully connected layers accordingly.\n",
    "\n",
    "Additionally, state-of-the-art CNN architectures like ResNet, Inception, and DenseNet use a lot of other strategies like skip connections, depth-wise separable convolutions, and densely connected layers to improve performance. These designs are often the result of extensive experimentation and research to optimize network performance on large-scale image recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes = 4):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, padding=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctscan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
